{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2295aee",
   "metadata": {},
   "source": [
    "### Messages\n",
    "\n",
    "Messages are the fundamental unit of context for models in Langchain.They represent the input and output of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d83663",
   "metadata": {},
   "source": [
    "carrying both the content and metadataa needed to represent the state of a conversation when interacting with a LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6dab3c",
   "metadata": {},
   "source": [
    "Messages are the objects that contain:\n",
    "\n",
    "1-Role: Identify the message type (e.g. system,user)                                                                                   \n",
    "2-Content: Represents the actual conent of the message (like text,images,audio,documents etc).                                        \n",
    "3-Metadata: optional field such as response information,message IDs and token usage\n",
    "\n",
    "Langchain provides a standard message type that works across all the model providers,ensuring consistent behavior regardless of the model being called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "738a88b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm functioning properly, thank you for asking. I'm a large language model, so I don't have feelings in the same way that humans do, but I'm here to help with any questions or tasks you may have. How can I assist you today?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 41, 'total_tokens': 95, 'completion_time': 0.06669106, 'completion_tokens_details': None, 'prompt_time': 0.001963217, 'prompt_tokens_details': None, 'queue_time': 0.046456373, 'total_time': 0.068654277}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c900f-f273-7b62-907f-727afc53c1e0-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 41, 'output_tokens': 54, 'total_tokens': 95})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model=init_chat_model(\"groq:llama-3.1-8b-instant\")\n",
    "model.invoke(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f734b9c",
   "metadata": {},
   "source": [
    "### Text prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2af77c",
   "metadata": {},
   "source": [
    "Use text prompt when:                                                                                                                   \n",
    "    1-you have a single standalone text                                                                                             \n",
    "    2-you dont need conversation history                                                                                                \n",
    "    3-you want minimal code complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f516367",
   "metadata": {},
   "source": [
    "#Message prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f52584",
   "metadata": {},
   "source": [
    "1-System Messgage                                                                                                                       \n",
    "2-Human Message                                                                                                                         \n",
    "3-AI Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0998d011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good day to you, Sangram. I am well, thank you for asking. I have a busy schedule ahead of me, with several cases to preside over today. But I am always happy to take a brief moment to speak with you. What brings you here today? Are you seeking some guidance or perhaps have a matter that requires my attention?\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import SystemMessage, HumanMessage,AIMessage\n",
    "\n",
    "\n",
    "system_message = SystemMessage(content=\"You are a judge. The user name is Sangram\")\n",
    "human_message = HumanMessage(content=\"Hello, how are you?\")\n",
    "\n",
    "messages = [system_message, human_message]\n",
    "metadata = {\"user_id\": \"1234567890\",\"user_name\":\"Sangram\"}\n",
    "\n",
    "response = model.invoke(\n",
    "    messages,\n",
    "    config={\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    ")\n",
    "print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a95f1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 52, 'output_tokens': 72, 'total_tokens': 124}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825f3dec",
   "metadata": {},
   "source": [
    "### Tool Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1bf653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI tool call: [{'name': 'add_numbers', 'args': {'a': 5, 'b': 3}, 'id': 'pgcavpz82', 'type': 'tool_call'}]\n",
      "Final Answer: The result of 5 + 3 is 8.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import tool\n",
    "from langchain.messages import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    ToolMessage\n",
    ")\n",
    "\n",
    "# 1️⃣ Define a tool\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers together.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "# 2️⃣ Create model with tool binding\n",
    "model_with_tools = model.bind_tools([add_numbers])\n",
    "\n",
    "\n",
    "# 3️⃣ Send user message\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"What is 5 + 3?\")\n",
    "]\n",
    "\n",
    "# 4️⃣ Model decides to call tool\n",
    "ai_msg = model_with_tools.invoke(messages)\n",
    "\n",
    "\n",
    "print(\"AI tool call:\", ai_msg.tool_calls)\n",
    "\n",
    "# 5️⃣ Execute tool manually\n",
    "tool_call = ai_msg.tool_calls[0]\n",
    "\n",
    "result = add_numbers.invoke(tool_call[\"args\"])\n",
    "\n",
    "# 6️⃣ Send ToolMessage back to model\n",
    "messages.append(ai_msg)\n",
    "\n",
    "messages.append(\n",
    "    ToolMessage(\n",
    "        content=str(result),\n",
    "        tool_call_id=tool_call[\"id\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "# 7️⃣ Final model response\n",
    "final_response = model_with_tools.invoke(messages)\n",
    "\n",
    "print(\"Final Answer:\", final_response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
